{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalacja i dołączenie zależności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq torch pandas numpy pytorch_lightning sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Standard data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from lib.utils import types_to_int, TYPES_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"./\" # @param {type: \"string\"}\n",
    "DATASET = \"headlines\" # @param {type: \"string\"}\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data/sem_eval_2016/\", DATASET)\n",
    "\n",
    "SBERT_EMBEDDING_WIDTH = 768\n",
    "\n",
    "TRAIN_VAL_SPLIT = 0.8 # @param {type: \"slider\", min:0, max: 1}\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16 # @param {type: \"slider\", min:1, max:128}\n",
    "BATCH_SIZE = 16 # @param {type: \"slider\", min:1, max:128}\n",
    "NUM_WORKERS = 0 # @param {type: \"slider\", min:1, max:16}\n",
    "PERSISTENT_WORKERS = False # TODO param\n",
    "\n",
    "EPOCHS = 10 # @param {type: \"slider\", min:1, max:128}\n",
    "ACCELERATOR = \"cpu\" # @param [\"auto\", \"gpu\", \"tpu\", \"cpu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moduł ładowania danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTDataset(Dataset):\n",
    "    def __init__(self, file_path: str):\n",
    "        self._data = pd.read_csv(\n",
    "            file_path, sep=\"\\t\", keep_default_na=False, quoting=csv.QUOTE_NONE\n",
    "        )\n",
    "\n",
    "        self._sbert = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "        self._types = self._get_encoded_types()\n",
    "        self._scores = torch.tensor(self._data[\"y_score\"]).float()\n",
    "\n",
    "    def _get_encoded_types(self):\n",
    "        types_as_int = types_to_int(self._data[\"y_type\"].tolist())\n",
    "        encoded_types = torch.nn.functional.one_hot(\n",
    "            torch.tensor(types_as_int), num_classes=len(TYPES_MAP)\n",
    "        ).float()\n",
    "        return encoded_types\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1 = self._data[\"x1\"]\n",
    "        x2 = self._data[\"x2\"]\n",
    "\n",
    "        a = self._sbert.encode(x1[index])\n",
    "        b = self._sbert.encode(x2[index])\n",
    "        c = np.concatenate((a, b))\n",
    "\n",
    "        x = torch.tensor(c)\n",
    "        y = (self._types[index], self._scores[index])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._types.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        batch_size: int,\n",
    "        train_batch_size: int,\n",
    "        num_workers: int,\n",
    "        persistent_workers: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._train_dataset = None\n",
    "        self._val_dataset = None\n",
    "        self._test_dataset = None\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._train_batch_size = train_batch_size\n",
    "        self._num_workers = num_workers\n",
    "        self._persistent_workers = persistent_workers\n",
    "        self._train_path = train_path\n",
    "        self._test_path = test_path\n",
    "        self._preparte_data_per_node = True\n",
    "\n",
    "    def _split(self, dataset, proportion):\n",
    "        a = int(len(dataset) * proportion)\n",
    "        b = len(dataset) - a\n",
    "        return random_split(dataset, (a, b))\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if self._train_dataset is not None:\n",
    "            return\n",
    "\n",
    "        self._train_dataset, self._val_dataset = self._split(\n",
    "            SBERTDataset(self._train_path), TRAIN_VAL_SPLIT\n",
    "        )\n",
    "        self._test_dataset = SBERTDataset(self._test_path)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._train_dataset,\n",
    "            batch_size=self._train_batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "            persistent_workers=self._persistent_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._val_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "            persistent_workers=self._persistent_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._test_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "            persistent_workers=self._persistent_workers,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._test_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "            shuffle=False,\n",
    "            persistent_workers=self._persistent_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pojedyncza warstwa, typ i dopasowanie uczone razem, model SBERT nie podlega treningowi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayeredHeadJointLearningWithSBERTFrozen(LightningModule):\n",
    "    def __init__(\n",
    "        self, sbert_model: str = \"all-mpnet-base-v2\", learning_rate: float = 0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._scoring_head = torch.nn.Linear(in_features=SBERT_EMBEDDING_WIDTH * 2, out_features=1)\n",
    "        self._class_head = torch.nn.Linear(\n",
    "            in_features=SBERT_EMBEDDING_WIDTH * 2, out_features=len(TYPES_MAP)\n",
    "        )\n",
    "        self._learning_rate = learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def _step(self, batch, batch_idx, id: str):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return self.loss(y, y_hat, id)\n",
    "\n",
    "    def forward(self, x):\n",
    "        score = torch.reshape(self._scoring_head(x), (-1,))\n",
    "        cls = torch.nn.functional.softmax(self._class_head(x), dim=1)\n",
    "\n",
    "        return cls, score\n",
    "\n",
    "    def loss(self, y, y_hat, id):\n",
    "        # Klasa i ocena uczone razem\n",
    "        scoring_loss = torch.nn.functional.mse_loss(y_hat[1], y[1])\n",
    "        class_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            y_hat[0], y[0]\n",
    "        )\n",
    "        return scoring_loss + class_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch, batch_idx, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch, batch_idx, \"val\")\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch, batch_idx, \"test\")\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        types, scores = self.forward(x)\n",
    "\n",
    "        return torch.argmax(types, dim=1), torch.clamp(\n",
    "            torch.round(scores).int(), min=0, max=5\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self._learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleLayeredHeadJointLearningWithSBERTFrozen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SBERTDataModule(\n",
    "    f'{DATA_DIR}/train.tsv', \n",
    "    f'{DATA_DIR}/test.tsv', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=PERSISTENT_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator=ACCELERATOR, max_epochs=EPOCHS, strategy=\"ddp_fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "  | Name          | Type   | Params\n",
      "-----------------------------------------\n",
      "0 | _scoring_head | Linear | 1.5 K \n",
      "1 | _class_head   | Linear | 12.3 K\n",
      "-----------------------------------------\n",
      "13.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.8 K    Total params\n",
      "0.055     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 645, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1098, in _run\n    results = self._run_stage()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1177, in _run_stage\n    self._run_train()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1190, in _run_train\n    self._run_sanity_check()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1255, in _run_sanity_check\n    val_loop._reload_evaluation_dataloaders()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 234, in _reload_evaluation_dataloaders\n    self.trainer.reset_val_dataloader()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1635, in reset_val_dataloader\n    self.num_val_batches, self.val_dataloaders = self._data_connector._reset_eval_dataloader(\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 357, in _reset_eval_dataloader\n    dataloaders = self._request_dataloader(mode)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 446, in _request_dataloader\n    dataloader = source.dataloader()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 524, in dataloader\n    return method()\n  File \"/tmp/ipykernel_57917/1552273051.py\", line 49, in val_dataloader\n    return DataLoader(\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/lightning_lite/utilities/data.py\", line 323, in wrapper\n    init(obj, *args, **kwargs)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 246, in __init__\n    raise ValueError('persistent_workers option needs num_workers > 0')\nValueError: persistent_workers option needs num_workers > 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch.multiprocessing.set_start_method('spawn')# good solution !!!! https://github.com/pytorch/pytorch/issues/40403\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:603\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 603\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    605\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:36\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mlauncher\u001b[39m.\u001b[39;49mlaunch(trainer_fn, \u001b[39m*\u001b[39;49margs, trainer\u001b[39m=\u001b[39;49mtrainer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:113\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     process_args \u001b[39m=\u001b[39m [trainer, function, args, kwargs, return_queue]\n\u001b[0;32m--> 113\u001b[0m mp\u001b[39m.\u001b[39;49mstart_processes(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapping_function,\n\u001b[1;32m    115\u001b[0m     args\u001b[39m=\u001b[39;49mprocess_args,\n\u001b[1;32m    116\u001b[0m     nprocs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49mnum_processes,\n\u001b[1;32m    117\u001b[0m     start_method\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_start_method,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m worker_output \u001b[39m=\u001b[39m return_queue\u001b[39m.\u001b[39mget()\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m trainer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 645, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1098, in _run\n    results = self._run_stage()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1177, in _run_stage\n    self._run_train()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1190, in _run_train\n    self._run_sanity_check()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1255, in _run_sanity_check\n    val_loop._reload_evaluation_dataloaders()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 234, in _reload_evaluation_dataloaders\n    self.trainer.reset_val_dataloader()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1635, in reset_val_dataloader\n    self.num_val_batches, self.val_dataloaders = self._data_connector._reset_eval_dataloader(\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 357, in _reset_eval_dataloader\n    dataloaders = self._request_dataloader(mode)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 446, in _request_dataloader\n    dataloader = source.dataloader()\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 524, in dataloader\n    return method()\n  File \"/tmp/ipykernel_57917/1552273051.py\", line 49, in val_dataloader\n    return DataLoader(\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/lightning_lite/utilities/data.py\", line 323, in wrapper\n    init(obj, *args, **kwargs)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 246, in __init__\n    raise ValueError('persistent_workers option needs num_workers > 0')\nValueError: persistent_workers option needs num_workers > 0\n"
     ]
    }
   ],
   "source": [
    "# torch.multiprocessing.set_start_method('spawn')# good solution !!!! https://github.com/pytorch/pytorch/issues/40403\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [04:21, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_46965/4014144725.py\", line 23, in __getitem__\n    a = self._sbert.encode(x1[index])\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 153, in encode\n    self.to(device)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 989, in to\n    return self._apply(convert)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 641, in _apply\n    module._apply(fn)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 641, in _apply\n    module._apply(fn)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 641, in _apply\n    module._apply(fn)\n  [Previous line repeated 1 more time]\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 664, in _apply\n    param_applied = fn(param)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 987, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 217, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:780\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.test()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    779\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> 780\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    781\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    782\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:829\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tested_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[39m# run test\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    831\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    832\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1098\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1096\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1098\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1100\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1174\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mdispatch(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_evaluate()\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1211\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage\u001b[39m}\u001b[39;00m\u001b[39m_evaluation\u001b[39m\u001b[39m\"\u001b[39m), _evaluation_context(\n\u001b[1;32m   1212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_mode\n\u001b[1;32m   1213\u001b[0m ):\n\u001b[0;32m-> 1214\u001b[0m     eval_loop_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1216\u001b[0m \u001b[39m# remove the tensors from the eval results\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m eval_loop_results:\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:121\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    120\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n\u001b[0;32m--> 121\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    266\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    281\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_46965/4014144725.py\", line 23, in __getitem__\n    a = self._sbert.encode(x1[index])\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 153, in encode\n    self.to(device)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 989, in to\n    return self._apply(convert)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 641, in _apply\n    module._apply(fn)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 641, in _apply\n    module._apply(fn)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 641, in _apply\n    module._apply(fn)\n  [Previous line repeated 1 more time]\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 664, in _apply\n    param_applied = fn(param)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 987, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n  File \"/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 217, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 199it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 128/128 [02:50<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_sep = ' // '\n",
    "\n",
    "def preds_to_wa(wa_content: str, preds_lines):\n",
    "    wa_lines = wa_content.splitlines()\n",
    "\n",
    "    idx = 0\n",
    "    result = []\n",
    "\n",
    "    for line in wa_lines:\n",
    "        line_res = line\n",
    "\n",
    "        if '<==>' in line:\n",
    "            fields = line.split(fields_sep)\n",
    "            preds_fields = preds_lines[idx].split()\n",
    "\n",
    "            fields[1] = preds_fields[1]\n",
    "            fields[2] = preds_fields[2]\n",
    "\n",
    "            line_res = fields_sep.join(fields)\n",
    "            idx += 1\n",
    "\n",
    "        result.append(line_res)\n",
    "    \n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "types_inv_map = {v: k for k, v in TYPES_MAP.items()}\n",
    "\n",
    "types = list(map(lambda t: types_inv_map[t], flatten([t.tolist() for t, s in predictions])))\n",
    "scores = flatten([s.tolist() for t, s in predictions])\n",
    "\n",
    "predictions = [\n",
    "    f\"{index}\\t{item[0]} {item[1]}\\n\" for index, item in enumerate(zip(types, scores))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_file = os.path.join(DATA_DIR, f\"STSint.testinput.{DATASET}.wa\")\n",
    "wa_output_file = os.path.join(DATA_DIR, f\"STSint.testinput.{DATASET}-predictions.wa\")\n",
    "\n",
    "with open(wa_file) as file:\n",
    "    wa_test = file.read()\n",
    "\n",
    "wa_predictions = preds_to_wa(wa_test, predictions)\n",
    "\n",
    "with open(wa_output_file, \"w\") as file:\n",
    "    file.write(wa_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing perl evalF1_penalty.pl ./data/sem_eval_2016/headlines/STSint.testinput.headlines.wa ./data/sem_eval_2016/headlines/STSint.testinput.headlines-predictions.wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk aligned twice 10 (3 4 5):7 8 9 10 <==> 3 4 5 // OPPO // 4 // instead of releasing them <==> Palestinian prisoner release  at evalF1_penalty.pl line 165, <I> line 322.\n",
      "chunk aligned twice 177 (1 2 3):1 2 3 <==> 1 2 3 // REL // 4 // Bangladesh collapse search <==> Bangladesh collapse deaths  at evalF1_penalty.pl line 165, <I> line 5913.\n",
      "chunk aligned twice 230 (1 2 3):1 2 <==> 1 2 3 // REL // 4 // Arsenal stars <==> Arsenal great Rice  at evalF1_penalty.pl line 165, <I> line 7658.\n",
      "chunk aligned twice 305 (3 4):4 <==> 3 4 // SIMI // 3 // who <==> Christian woman  at evalF1_penalty.pl line 165, <I> line 10154.\n",
      "chunk aligned twice 362 (6):1 <==> 6 // EQUI // 5 // Gunman <==> Gunman  at evalF1_penalty.pl line 165, <I> line 12076.\n",
      "chunk aligned twice 10 (3 4 5):7 8 9 10 <==> 3 4 5 // EQUI // 2 // instead of releasing them <==> Palestinian prisoner release  at evalF1_penalty.pl line 165, <I> line 12836.\n",
      "chunk aligned twice 177 (1 2 3):1 2 3 <==> 1 2 3 // EQUI // 2 // Bangladesh collapse search <==> Bangladesh collapse deaths  at evalF1_penalty.pl line 165, <I> line 18427.\n",
      "chunk aligned twice 230 (1 2 3):1 2 <==> 1 2 3 // EQUI // 2 // Arsenal stars <==> Arsenal great Rice  at evalF1_penalty.pl line 165, <I> line 20172.\n",
      "chunk aligned twice 305 (3 4):4 <==> 3 4 // EQUI // 2 // who <==> Christian woman  at evalF1_penalty.pl line 165, <I> line 22668.\n",
      "chunk aligned twice 362 (6):1 <==> 6 // EQUI // 2 // Gunman <==> Gunman  at evalF1_penalty.pl line 165, <I> line 24590.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " F1 Ali     1.0000\n",
      " F1 Type    0.5552\n",
      " F1 Score   0.5399\n",
      " F1 Typ+Sco 0.3545\n",
      "\n",
      "Executing perl evalF1_no_penalty.pl ./data/sem_eval_2016/headlines/STSint.testinput.headlines.wa ./data/sem_eval_2016/headlines/STSint.testinput.headlines-predictions.wa\n",
      " F1 Ali     1.0000\n",
      " F1 Type    0.5552\n",
      " F1 Score   0.5399\n",
      " F1 Typ+Sco 0.2382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "cmds = [\n",
    "    f\"perl evalF1_penalty.pl {wa_file} {wa_output_file}\",\n",
    "    f\"perl evalF1_no_penalty.pl {wa_file} {wa_output_file}\",\n",
    "]\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(f\"Executing {cmd}\")\n",
    "    print(check_output(cmd.split(), cwd=\"./\").decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddddd67d25b748c78a550d661803566ec92041f34219f81d19ad22bc923c0e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
