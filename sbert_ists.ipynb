{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3907.78s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq torch pandas numpy pytorch_lightning sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Standard data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from lib.utils import types_to_int, TYPES_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"./\" # @param {type: \"string\"}\n",
    "DATASET = \"headlines\" # @param {type: \"string\"}\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data/sem_eval_2016/\", DATASET)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16 # @param {type: \"slider\", min:1, max:128}\n",
    "BATCH_SIZE = 16 # @param {type: \"slider\", min:1, max:128}\n",
    "NUM_WORKERS = 2 # @param {type: \"slider\", min:1, max:16}\n",
    "\n",
    "EPOCHS = 1 # @param {type: \"slider\", min:1, max:128}\n",
    "ACCELERATOR = \"auto\" # @param [\"auto\", \"gpu\", \"tpu\", \"cpu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTDataset(Dataset):\n",
    "    def __init__(self, file_path: str):\n",
    "        self._data = pd.read_csv(\n",
    "            file_path, sep=\"\\t\", keep_default_na=False, quoting=csv.QUOTE_NONE\n",
    "        )\n",
    "\n",
    "        self._sbert = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "        self._types = self._get_encoded_types()\n",
    "        self._scores = torch.tensor(self._data[\"y_score\"]).float()\n",
    "\n",
    "    def _get_encoded_types(self):\n",
    "        types_as_int = types_to_int(self._data[\"y_type\"].tolist())\n",
    "        encoded_types = torch.nn.functional.one_hot(\n",
    "            torch.tensor(types_as_int), num_classes=len(TYPES_MAP)\n",
    "        ).float()\n",
    "        return encoded_types\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1 = self._data[\"x1\"]\n",
    "        x2 = self._data[\"x2\"]\n",
    "\n",
    "        a = self._sbert.encode(x1[index])\n",
    "        b = self._sbert.encode(x2[index])\n",
    "        c = np.concatenate((a, b))\n",
    "\n",
    "        x = torch.tensor(c)\n",
    "        y = (self._types[index], self._scores[index])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._types.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_VAL_SPLIT = 0.8\n",
    "\n",
    "\n",
    "class SBERTDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        batch_size: int,\n",
    "        train_batch_size: int,\n",
    "        num_workers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._train_dataset = None\n",
    "        self._val_dataset = None\n",
    "        self._test_dataset = None\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._train_batch_size = train_batch_size\n",
    "        self._num_workers = num_workers\n",
    "        self._train_path = train_path\n",
    "        self._test_path = test_path\n",
    "        self._preparte_data_per_node = True\n",
    "\n",
    "    def _split(self, dataset, proportion):\n",
    "        a = int(len(dataset) * proportion)\n",
    "        b = len(dataset) - a\n",
    "        return random_split(dataset, (a, b))\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if self._train_dataset is not None:\n",
    "            return\n",
    "\n",
    "        self._train_dataset, self._val_dataset = self._split(\n",
    "            SBERTDataset(self._train_path), TRAIN_VAL_SPLIT\n",
    "        )\n",
    "        self._test_dataset = SBERTDataset(self._test_path)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._train_dataset,\n",
    "            batch_size=self._train_batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._val_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._test_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self._test_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            num_workers=self._num_workers,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERT_iSTS_Model(LightningModule):\n",
    "    def __init__(\n",
    "        self, sbert_model: str = \"all-mpnet-base-v2\", learning_rate: float = 0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # kolasdam(TODO) sparametryzować\n",
    "        self._scoring_head = torch.nn.Linear(in_features=768 * 2, out_features=1)\n",
    "        self._class_head = torch.nn.Linear(\n",
    "            in_features=768 * 2, out_features=len(TYPES_MAP)\n",
    "        )\n",
    "        self._learning_rate = learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def _step(self, batch, batch_idx, id: str):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return self.loss(y, y_hat, id)\n",
    "\n",
    "    def forward(self, x):\n",
    "        score = torch.reshape(self._scoring_head(x), (-1,))\n",
    "        cls = torch.nn.functional.softmax(self._class_head(x), dim=1)\n",
    "\n",
    "        return cls, score\n",
    "\n",
    "    def loss(self, y, y_hat, id):\n",
    "        # Klasa i ocena uczone razem\n",
    "        scoring_loss = torch.nn.functional.mse_loss(y_hat[1], y[1])\n",
    "        class_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            y_hat[0], y[0]\n",
    "        )\n",
    "        return scoring_loss + class_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch, batch_idx, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch, batch_idx, \"val\")\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch, batch_idx, \"test\")\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        types, scores = self.forward(x)\n",
    "\n",
    "        return torch.argmax(types, dim=1), torch.clamp(\n",
    "            torch.round(scores).int(), min=0, max=5\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self._learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SBERT_iSTS_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SBERTDataModule(\n",
    "    f'{DATA_DIR}/train.tsv', \n",
    "    f'{DATA_DIR}/test.tsv', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator=ACCELERATOR, max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type   | Params\n",
      "-----------------------------------------\n",
      "0 | _scoring_head | Linear | 1.5 K \n",
      "1 | _class_head   | Linear | 12.3 K\n",
      "-----------------------------------------\n",
      "13.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.8 K    Total params\n",
      "0.055     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/249 [00:00<?, ?it/s] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 0:  80%|███████▉  | 199/249 [04:22<01:05,  1.32s/it, loss=4.5, v_num=16] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 0: 100%|██████████| 249/249 [05:29<00:00,  1.32s/it, loss=4.5, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 249/249 [05:29<00:00,  1.32s/it, loss=4.5, v_num=16]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Testing DataLoader 0: 100%|██████████| 128/128 [02:38<00:00,  1.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damiankolaska/Desktop/NLP_SBERT_interpretable_semantic_text_similarity/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 199it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 128/128 [02:50<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_sep = ' // '\n",
    "\n",
    "def preds_to_wa(wa_content: str, preds_lines):\n",
    "    wa_lines = wa_content.splitlines()\n",
    "\n",
    "    idx = 0\n",
    "    result = []\n",
    "\n",
    "    for line in wa_lines:\n",
    "        line_res = line\n",
    "\n",
    "        if '<==>' in line:\n",
    "            fields = line.split(fields_sep)\n",
    "            preds_fields = preds_lines[idx].split()\n",
    "\n",
    "            fields[1] = preds_fields[1]\n",
    "            fields[2] = preds_fields[2]\n",
    "\n",
    "            line_res = fields_sep.join(fields)\n",
    "            idx += 1\n",
    "\n",
    "        result.append(line_res)\n",
    "    \n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "types_inv_map = {v: k for k, v in TYPES_MAP.items()}\n",
    "\n",
    "types = list(map(lambda t: types_inv_map[t], flatten([t.tolist() for t, s in predictions])))\n",
    "scores = flatten([s.tolist() for t, s in predictions])\n",
    "\n",
    "predictions = [\n",
    "    f\"{index}\\t{item[0]} {item[1]}\\n\" for index, item in enumerate(zip(types, scores))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_file = os.path.join(DATA_DIR, f\"STSint.testinput.{DATASET}.wa\")\n",
    "wa_output_file = os.path.join(DATA_DIR, f\"STSint.testinput.{DATASET}-predictions.wa\")\n",
    "\n",
    "with open(wa_file) as file:\n",
    "    wa_test = file.read()\n",
    "\n",
    "wa_predictions = preds_to_wa(wa_test, predictions)\n",
    "\n",
    "with open(wa_output_file, \"w\") as file:\n",
    "    file.write(wa_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing perl evalF1_penalty.pl ./data/sem_eval_2016/headlines/STSint.testinput.headlines.wa ./data/sem_eval_2016/headlines/STSint.testinput.headlines-predictions.wa\n",
      "Epoch 1:  35%|███▌      | 88/249 [16:00<29:16, 10.91s/it, loss=3.61, v_num=15]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Mazury'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cmd \u001b[38;5;129;01min\u001b[39;00m cmds:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/Mazury\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode())\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:420\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         empty \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    418\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m empty\n\u001b[0;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m run(\u001b[39m*\u001b[39;49mpopenargs, stdout\u001b[39m=\u001b[39;49mPIPE, timeout\u001b[39m=\u001b[39;49mtimeout, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    421\u001b[0m            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mstdout\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:501\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstdout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[1;32m    499\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[0;32m--> 501\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    502\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m, timeout\u001b[39m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:969\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    966\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    967\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 969\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    970\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    971\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    972\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    973\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    974\u001b[0m                         errread, errwrite,\n\u001b[1;32m    975\u001b[0m                         restore_signals,\n\u001b[1;32m    976\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    977\u001b[0m                         start_new_session)\n\u001b[1;32m    978\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1845\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1843\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1844\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1845\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1846\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Mazury'"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "cmds = [\n",
    "    f\"perl evalF1_penalty.pl {wa_file} {wa_output_file}\",\n",
    "    f\"perl evalF1_no_penalty.pl {wa_file} {wa_output_file}\",\n",
    "]\n",
    "\n",
    "for cmd in cmds:\n",
    "    print(f\"Executing {cmd}\")\n",
    "    print(check_output(cmd.split(), cwd=\"./\").decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddddd67d25b748c78a550d661803566ec92041f34219f81d19ad22bc923c0e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
